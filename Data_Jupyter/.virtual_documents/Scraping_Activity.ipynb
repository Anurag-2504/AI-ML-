#web Scraping

import requests
from bs4 import BeautifulSoup


# Fetch and store it

page_count=1

while True:
    URL = f"https://quotes.toscrape.com/page/{page_count}/"
    res =  requests.get(URL)

    soup = BeautifulSoup(res.text,"lxml")
    quotes = soup.select("div.quote")

    if not quotes:
        print("No Valid Pages...")
        break
    with open(f"scraped_data/quotes{page_count}.html","w") as f:
        f.write(res.text)
        print(f"downloaded data successfully for page {page_count}")
    page_count = page_count+1



#Extract useful information for pages

life_quotes = []

for i in range(10):
    with open(f"scraped_data/quotes{i+1}.html", "r") as f:
        html_content = f.read()

    soup = BeautifulSoup(html_content, "lxml")
    all_quotes = soup.select("div.quote")

    for q in all_quotes:
        tags = [tag.get_text() for tag in q.select(".tags .tag")]

        if "life" in tags:
            text = q.select_one("span.text").get_text()
            author = q.select_one("small.author").get_text()
            life_quotes.append([text, author])

print(life_quotes)



import pandas as pd

df = pd.DataFrame(life_quotes, columns=["Quote", "Author"])
df.to_csv("cleaned_data/life_quotes.csv", index=False, encoding="utf-8")

print("CSV file created successfully!")

